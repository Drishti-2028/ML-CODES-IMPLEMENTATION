{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML3yFu3OEN9tp2wOja9gfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drishti-2028/ML-CODES-IMPLEMENTATION/blob/main/decisiontree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJB2QdEXYJ9Z"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------\n",
        "# ID3 DECISION TREE ALGORITHM\n",
        "# Full Algorithm + Code Combined\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Function to calculate Entropy\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def entropy(data):\n",
        "    \"\"\"\n",
        "    Step 1:\n",
        "    Compute entropy of given dataset.\n",
        "    Entropy(S) = - sum(p_i * log2(p_i))\n",
        "    \"\"\"\n",
        "    labels = [row[-1] for row in data]\n",
        "    label_counts = Counter(labels)\n",
        "\n",
        "    total = len(data)\n",
        "    ent = 0\n",
        "\n",
        "    for count in label_counts.values():\n",
        "        p = count / total\n",
        "        ent -= p * math.log2(p)\n",
        "\n",
        "    return ent\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Function to split dataset based on attribute\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def split_data(data, attribute_index, value):\n",
        "    \"\"\"\n",
        "    Step 2:\n",
        "    Create subset where attribute = value\n",
        "    \"\"\"\n",
        "    subset = []\n",
        "    for row in data:\n",
        "        if row[attribute_index] == value:\n",
        "            reduced_row = row[:attribute_index] + row[attribute_index+1:]\n",
        "            subset.append(reduced_row)\n",
        "    return subset\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Function to compute Information Gain\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def information_gain(data, attribute_index):\n",
        "    \"\"\"\n",
        "    Step 3:\n",
        "    Gain(S,A) = Entropy(S) - Weighted Entropy of subsets\n",
        "    \"\"\"\n",
        "    total_entropy = entropy(data)\n",
        "\n",
        "    values = set([row[attribute_index] for row in data])\n",
        "    weighted_entropy = 0\n",
        "    total = len(data)\n",
        "\n",
        "    for value in values:\n",
        "        subset = split_data(data, attribute_index, value)\n",
        "        weight = len(subset) / total\n",
        "        weighted_entropy += weight * entropy(subset)\n",
        "\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ID3 Recursive Algorithm\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def id3(data, attributes):\n",
        "\n",
        "    # Step 4: If all examples have same class → return class\n",
        "    labels = [row[-1] for row in data]\n",
        "    if labels.count(labels[0]) == len(labels):\n",
        "        return labels[0]\n",
        "\n",
        "    # Step 5: If no attributes left → return majority class\n",
        "    if len(attributes) == 0:\n",
        "        return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "    # Step 6: Select attribute with highest information gain\n",
        "    gains = [information_gain(data, i) for i in range(len(attributes))]\n",
        "    best_attr_index = gains.index(max(gains))\n",
        "    best_attr = attributes[best_attr_index]\n",
        "\n",
        "    tree = {best_attr: {}}\n",
        "\n",
        "    # Step 7: For each value of best attribute\n",
        "    values = set([row[best_attr_index] for row in data])\n",
        "\n",
        "    for value in values:\n",
        "        subset = split_data(data, best_attr_index, value)\n",
        "\n",
        "        if not subset:\n",
        "            tree[best_attr][value] = Counter(labels).most_common(1)[0][0]\n",
        "        else:\n",
        "            remaining_attrs = attributes[:best_attr_index] + attributes[best_attr_index+1:]\n",
        "            tree[best_attr][value] = id3(subset, remaining_attrs)\n",
        "\n",
        "    return tree\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Example Usage\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Dataset: Outlook, Temperature, Humidity, Wind, Play\n",
        "    data = [\n",
        "        ['Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
        "        ['Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
        "        ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
        "        ['Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
        "        ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "        ['Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
        "        ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes']\n",
        "    ]\n",
        "\n",
        "    attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "    tree = id3(data, attributes)\n",
        "\n",
        "    print(\"Decision Tree:\")\n",
        "    print(tree)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# NAIVE BAYES CLASSIFIER (Categorical Data)\n",
        "# Full Algorithm + Code Combined\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "class NaiveBayes:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.class_priors = {}\n",
        "        self.likelihoods = {}\n",
        "        self.classes = None\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # Step 1: Train the model\n",
        "    # -------------------------------------------------------\n",
        "    def fit(self, X, Y):\n",
        "\n",
        "        n = len(Y)\n",
        "        self.classes = set(Y)\n",
        "\n",
        "        # Step 1: Compute Prior Probabilities P(C)\n",
        "        class_counts = Counter(Y)\n",
        "\n",
        "        for c in self.classes:\n",
        "            self.class_priors[c] = class_counts[c] / n\n",
        "\n",
        "        # Step 2: Compute Likelihood P(x_i | C)\n",
        "        self.likelihoods = {c: defaultdict(lambda: defaultdict(int)) for c in self.classes}\n",
        "\n",
        "        for features, label in zip(X, Y):\n",
        "            for i, value in enumerate(features):\n",
        "                self.likelihoods[label][i][value] += 1\n",
        "\n",
        "        # Convert counts to probabilities\n",
        "        for c in self.classes:\n",
        "            total_class_count = class_counts[c]\n",
        "            for feature_index in self.likelihoods[c]:\n",
        "                for value in self.likelihoods[c][feature_index]:\n",
        "                    self.likelihoods[c][feature_index][value] /= total_class_count\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # Step 3: Predict\n",
        "    # -------------------------------------------------------\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "\n",
        "        for sample in X:\n",
        "            class_scores = {}\n",
        "\n",
        "            for c in self.classes:\n",
        "\n",
        "                # Start with prior probability\n",
        "                score = self.class_priors[c]\n",
        "\n",
        "                # Multiply likelihoods\n",
        "                for i, value in enumerate(sample):\n",
        "\n",
        "                    if value in self.likelihoods[c][i]:\n",
        "                        score *= self.likelihoods[c][i][value]\n",
        "                    else:\n",
        "                        score *= 1e-6  # small value for unseen feature\n",
        "\n",
        "                class_scores[c] = score\n",
        "\n",
        "            # Step 4: Select class with highest probability\n",
        "            predicted_class = max(class_scores, key=class_scores.get)\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Example Usage\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Training Data (Categorical)\n",
        "    X_train = [\n",
        "        ['Sunny', 'Hot', 'High'],\n",
        "        ['Sunny', 'Hot', 'High'],\n",
        "        ['Overcast', 'Hot', 'High'],\n",
        "        ['Rain', 'Mild', 'High'],\n",
        "        ['Rain', 'Cool', 'Normal'],\n",
        "        ['Rain', 'Cool', 'Normal'],\n",
        "        ['Overcast', 'Cool', 'Normal']\n",
        "    ]\n",
        "\n",
        "    Y_train = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
        "\n",
        "    model = NaiveBayes()\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    test_sample = [['Sunny', 'Cool', 'High']]\n",
        "\n",
        "    prediction = model.predict(test_sample)\n",
        "\n",
        "    print(\"Predicted Class:\", prediction[0])"
      ],
      "metadata": {
        "id": "WXE-eucIZPQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}